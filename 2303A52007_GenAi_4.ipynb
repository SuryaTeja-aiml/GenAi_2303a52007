{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+U3pMpLeBIcfDQiLy43S5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SuryaTeja-aiml/GenAi_2303a52007/blob/main/2303a52007_GenAi_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Task 1**:\n",
        "\n",
        "1. Design a simple Artificial Neural Network (ANN) architecture with one input layer and one output layer (no hidden layer). Assume a linear activation function in the output layer.\n",
        "   \n",
        "2. Write Python code for a backpropagation algorithm with gradient descent optimization to update weights and bias parameters of the ANN model with the training data shown in Table 1.\n",
        "\n",
        "3. Calculate the mean square error (MSE) with the training and testing data shown in Table 2.\n",
        "\n",
        "4. Write Python code that reads the input data `[x1, x2, x3]` from the user. Predict the output with the deployed ANN model.\n",
        "\n",
        "---\n",
        "\n",
        "**Table 1: Training Data**\n",
        "\n",
        "| x1  | x2  | x3  | y    |\n",
        "|------|------|------|------|\n",
        "| 0.1  | 0.2  | 0.3  | 0.14 |\n",
        "| 0.2  | 0.3  | 0.4  | 0.20 |\n",
        "| 0.3  | 0.4  | 0.5  | 0.26 |\n",
        "| 0.5  | 0.6  | 0.7  | 0.38 |\n",
        "| 0.1  | 0.3  | 0.5  | 0.22 |\n",
        "| 0.2  | 0.4  | 0.6  | 0.28 |\n",
        "| 0.3  | 0.5  | 0.7  | 0.34 |\n",
        "| 0.4  | 0.6  | 0.8  | 0.40 |\n",
        "| 0.5  | 0.7  | 0.1  | 0.22 |\n",
        "\n",
        "---\n",
        "\n",
        "**Table 2: Test Data**\n",
        "\n",
        "| x1  | x2  | x3  | y    |\n",
        "|------|------|------|------|\n",
        "| 0.6  | 0.7  | 0.8  | 0.44 |\n",
        "| 0.7  | 0.8  | 0.9  | 0.50 |\n",
        "\n",
        "---\n",
        "\n",
        "**Instructions**:\n",
        "- Implement the ANN using Python code.\n",
        "- Display MSE results for both training and testing data.\n",
        "- Allow user input for `[x1, x2, x3]` to predict the output using the trained model.\n"
      ],
      "metadata": {
        "id": "pNBblZNVUOET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "X_train = np.array([\n",
        "    [0.1, 0.2, 0.3],\n",
        "    [0.2, 0.3, 0.4],\n",
        "    [0.3, 0.4, 0.5],\n",
        "    [0.5, 0.6, 0.7],\n",
        "    [0.1, 0.3, 0.5],\n",
        "    [0.2, 0.4, 0.6],\n",
        "    [0.3, 0.5, 0.7],\n",
        "    [0.4, 0.6, 0.8],\n",
        "    [0.5, 0.7, 0.1]\n",
        "])\n",
        "\n",
        "y_train = np.array([\n",
        "    [0.14],\n",
        "    [0.20],\n",
        "    [0.26],\n",
        "    [0.38],\n",
        "    [0.22],\n",
        "    [0.28],\n",
        "    [0.34],\n",
        "    [0.40],\n",
        "    [0.22]\n",
        "])\n",
        "\n",
        "X_test = np.array([\n",
        "    [0.6, 0.7, 0.8],\n",
        "    [0.7, 0.8, 0.9]\n",
        "])\n",
        "\n",
        "y_test = np.array([\n",
        "    [0.44],\n",
        "    [0.50]\n",
        "])\n",
        "\n",
        "W = np.random.randn(3, 1)\n",
        "b = np.random.randn(1)\n",
        "learning_rate = 0.01\n",
        "epochs = 1000\n",
        "\n",
        "def linear_activation(X, W, b):\n",
        "    return np.dot(X, W) + b\n",
        "\n",
        "def mse_loss(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    y_pred = linear_activation(X_train, W, b)\n",
        "    loss = mse_loss(y_train, y_pred)\n",
        "    dW = -(2 / X_train.shape[0]) * np.dot(X_train.T, (y_train - y_pred))\n",
        "    db = -(2 / X_train.shape[0]) * np.sum(y_train - y_pred)\n",
        "    W -= learning_rate * dW\n",
        "    b -= learning_rate * db\n",
        "\n",
        "y_train_pred = linear_activation(X_train, W, b)\n",
        "train_mse = mse_loss(y_train, y_train_pred)\n",
        "\n",
        "y_test_pred = linear_activation(X_test, W, b)\n",
        "test_mse = mse_loss(y_test, y_test_pred)\n",
        "\n",
        "print(f\"Training MSE: {train_mse}\")\n",
        "print(f\"Testing MSE: {test_mse}\")\n",
        "\n",
        "def predict(x1, x2, x3):\n",
        "    input_data = np.array([x1, x2, x3]).reshape(1, -1)\n",
        "    output = linear_activation(input_data, W, b)\n",
        "    return output[0][0]\n",
        "\n",
        "x1, x2, x3 = map(float, input(\"Enter values for x1, x2, x3: \").split())\n",
        "prediction = predict(x1, x2, x3)\n",
        "print(f\"Predicted output: {prediction}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fv66S8WWPvBi",
        "outputId": "ac661e7e-a8c8-4790-aa7b-338c8a852a13"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training MSE: 0.009696731424755764\n",
            "Testing MSE: 0.08538822590599893\n",
            "Enter values for x1, x2, x3: 0.1 0.2 0.3\n",
            "Predicted output: 0.0747976097229041\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2**:\n",
        "\n",
        "1. Design a simple Artificial Neural Network (ANN) architecture with **one input layer** and **one output layer** (no hidden layer). Assume a **sigmoid activation function** in the output layer:  \n",
        "   \\[\n",
        "   f(x) = 1/(1 + e^(-x))\n",
        "   \\]\n",
        "\n",
        "2. Write Python code for a **backpropagation algorithm** with **gradient descent optimization** to update weights and bias parameters of the ANN model using the **training data** in Table 3.\n",
        "\n",
        "3. Calculate the **Mean Squared Error (MSE)** with the **training** and **test data** in Table 4.\n",
        "\n",
        "4. Write Python code that:\n",
        "   - Accepts user input for \\([x1, x2, x3]\\).\n",
        "   - Predicts the output using the trained ANN model.\n",
        "\n",
        "---\n",
        "\n",
        "**Table 3: Training Data**\n",
        "\n",
        "| **x1** | **x2** | **x3** | **y**    |\n",
        "|--------|--------|--------|----------|\n",
        "| 0.1    | 0.2    | 0.3    | 0.5349   |\n",
        "| 0.2    | 0.3    | 0.4    | 0.5498   |\n",
        "| 0.3    | 0.4    | 0.5    | 0.5646   |\n",
        "| 0.5    | 0.6    | 0.7    | 0.5939   |\n",
        "| 0.1    | 0.3    | 0.5    | 0.5548   |\n",
        "| 0.2    | 0.4    | 0.6    | 0.5695   |\n",
        "| 0.3    | 0.5    | 0.7    | 0.5842   |\n",
        "| 0.4    | 0.6    | 0.8    | 0.5987   |\n",
        "| 0.5    | 0.7    | 0.1    | 0.5548   |\n",
        "\n",
        "---\n",
        "\n",
        "**Table 4: Test Data**\n",
        "\n",
        "| **x1** | **x2** | **x3** | **y**    |\n",
        "|--------|--------|--------|----------|\n",
        "| 0.6    | 0.7    | 0.8    | 0.6083   |\n",
        "| 0.7    | 0.8    | 0.9    | 0.6225   |\n",
        "\n",
        "---\n",
        "\n",
        "**Instructions**:\n",
        "- Design the ANN architecture without hidden layers and use the sigmoid activation function.\n",
        "- Implement the **backpropagation algorithm** for weight and bias updates.\n",
        "- Compute and display the **MSE** for both training and test datasets.\n",
        "- Allow the user to input \\([x1, x2, x3]\\) and predict the output using the trained ANN.\n",
        "`"
      ],
      "metadata": {
        "id": "aS25-_2Ou9QE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "X_train = np.array([\n",
        "    [0.1, 0.2, 0.3],\n",
        "    [0.2, 0.3, 0.4],\n",
        "    [0.3, 0.4, 0.5],\n",
        "    [0.5, 0.6, 0.7],\n",
        "    [0.1, 0.3, 0.5],\n",
        "    [0.2, 0.4, 0.6],\n",
        "    [0.3, 0.5, 0.7],\n",
        "    [0.4, 0.6, 0.8],\n",
        "    [0.5, 0.7, 0.1]\n",
        "])\n",
        "\n",
        "y_train = np.array([\n",
        "    [0.5349],\n",
        "    [0.5498],\n",
        "    [0.5646],\n",
        "    [0.5939],\n",
        "    [0.5548],\n",
        "    [0.5695],\n",
        "    [0.5842],\n",
        "    [0.5987],\n",
        "    [0.5548]\n",
        "])\n",
        "\n",
        "X_test = np.array([\n",
        "    [0.6, 0.7, 0.8],\n",
        "    [0.7, 0.8, 0.9]\n",
        "])\n",
        "\n",
        "y_test = np.array([\n",
        "    [0.6083],\n",
        "    [0.6225]\n",
        "])\n",
        "\n",
        "np.random.seed(0)\n",
        "W = np.random.randn(3, 1)\n",
        "b = np.random.randn(1)\n",
        "\n",
        "learning_rate = 0.01\n",
        "epochs = 1000\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    z = np.dot(X_train, W) + b\n",
        "    y_pred = sigmoid(z)\n",
        "\n",
        "    loss = np.mean((y_train - y_pred) ** 2)\n",
        "\n",
        "    d_loss_y_pred = 2 * (y_pred - y_train) / y_train.size\n",
        "    d_y_pred_z = sigmoid_derivative(z)\n",
        "    d_z_W = X_train\n",
        "    d_z_b = 1\n",
        "\n",
        "    d_loss_W = np.dot(d_z_W.T, d_loss_y_pred * d_y_pred_z)\n",
        "    d_loss_b = np.sum(d_loss_y_pred * d_y_pred_z, axis=0)\n",
        "\n",
        "    W -= learning_rate * d_loss_W\n",
        "    b -= learning_rate * d_loss_b\n",
        "\n",
        "train_loss = np.mean((y_train - sigmoid(np.dot(X_train, W) + b)) ** 2)\n",
        "test_loss = np.mean((y_test - sigmoid(np.dot(X_test, W) + b)) ** 2)\n",
        "\n",
        "print(f\"Training MSE: {train_loss}\")\n",
        "print(f\"Testing MSE: {test_loss}\")\n",
        "\n",
        "def predict(x1, x2, x3):\n",
        "    input_data = np.array([x1, x2, x3]).reshape(1, -1)\n",
        "    output = sigmoid(np.dot(input_data, W) + b)\n",
        "    return output[0][0]\n",
        "\n",
        "x1, x2, x3 = map(float, input(\"Enter values for x1, x2, x3: \").split())\n",
        "prediction = predict(x1, x2, x3)\n",
        "print(f\"Predicted output: {prediction}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2X6NYNNvzky",
        "outputId": "e4dec49c-0d79-4a87-e6e9-32002e9ed58a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training MSE: 0.1454359968769399\n",
            "Testing MSE: 0.13356397350569918\n",
            "Enter values for x1, x2, x3: 0.1 0.2 0.3\n",
            "Predicted output: 0.9172609659073521\n"
          ]
        }
      ]
    }
  ]
}
